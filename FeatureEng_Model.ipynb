{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andicuko/anaconda2/lib/python2.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"featurized_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'molecule_name',\n",
       " 'atom_index_0',\n",
       " 'atom_index_1',\n",
       " 'type',\n",
       " 'scalar_coupling_constant',\n",
       " 'atom_1_type',\n",
       " 'atom_1_hybridization',\n",
       " 'pi_bonds',\n",
       " 'graph_distance',\n",
       " 'graph_smile',\n",
       " 'angle',\n",
       " 'dihedral',\n",
       " 'sum_electronegativity_inbetwen',\n",
       " 'sum_electronegativity_neghb',\n",
       " 'donor_groups_in_neighb',\n",
       " 'aceptor_groups_in_neighb',\n",
       " 'posIonizable_groups_in_neighb',\n",
       " 'aromatic_groups_in_neighb',\n",
       " 'hydrophobe_groups_in_neighb',\n",
       " 'lumpedHydrophobe_groups_in_neighb',\n",
       " 'negIonizable_groups_in_neighb',\n",
       " 'sigma_bonds']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_encode\n",
    "to_meanencode = [\n",
    " 'atom_1_type',\n",
    " 'atom_1_hybridization',\n",
    " 'pi_bonds',\n",
    " 'graph_smile',\n",
    " 'donor_groups_in_neighb',\n",
    " 'aceptor_groups_in_neighb',\n",
    " 'posIonizable_groups_in_neighb',\n",
    " 'aromatic_groups_in_neighb',\n",
    " 'hydrophobe_groups_in_neighb',\n",
    " 'lumpedHydrophobe_groups_in_neighb',\n",
    " 'negIonizable_groups_in_neighb',\n",
    " 'sigma_bonds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom_1_type\n",
      "atom_1_hybridization\n",
      "pi_bonds\n",
      "graph_smile\n",
      "donor_groups_in_neighb\n",
      "aceptor_groups_in_neighb\n",
      "posIonizable_groups_in_neighb\n",
      "aromatic_groups_in_neighb\n",
      "hydrophobe_groups_in_neighb\n",
      "lumpedHydrophobe_groups_in_neighb\n",
      "negIonizable_groups_in_neighb\n",
      "sigma_bonds\n"
     ]
    }
   ],
   "source": [
    "mean = train.groupby([\"type\"]).agg({'scalar_coupling_constant': ['mean']})\n",
    "mean.columns = [ \"mean\"]\n",
    "mean.reset_index(inplace=True)\n",
    "for i in to_meanencode:\n",
    "    print i \n",
    "    gg = train.groupby([i, \"type\"]).agg({'scalar_coupling_constant': ['mean', 'count']})\n",
    "    gg.columns = [ i + '_mean', i + '_count' ]\n",
    "    gg.reset_index(inplace=True)\n",
    "    \n",
    "    tmp = pd.merge(gg, mean, on=[\"type\"], how=\"left\")\n",
    "    \n",
    "    m = 40\n",
    "    counts = tmp[i + '_count']\n",
    "    means = tmp[i + '_mean']\n",
    "    smooth = (counts * means + m * tmp['mean']) / (counts + m)\n",
    "    smooth\n",
    "    tmp[i+'_smthmean'] = smooth\n",
    "    train = pd.merge(train, tmp[[i, \"type\", i+\"_smthmean\"]], on=[i, \"type\"], how=\"left\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "atom_1_type\n",
      "atom_1_hybridization\n",
      "graph_smile\n"
     ]
    }
   ],
   "source": [
    "to_leableencode = [\n",
    "    \"type\",\n",
    "    'atom_1_type',\n",
    "    'atom_1_hybridization',\n",
    "    'graph_smile']\n",
    "for i in to_leableencode:\n",
    "    print i \n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[i])\n",
    "    train[i] = le.transform(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'molecule_name',\n",
       " 'atom_index_0',\n",
       " 'atom_index_1',\n",
       " 'type',\n",
       " 'scalar_coupling_constant',\n",
       " 'atom_1_type',\n",
       " 'atom_1_hybridization',\n",
       " 'pi_bonds',\n",
       " 'graph_distance',\n",
       " 'graph_smile',\n",
       " 'angle',\n",
       " 'dihedral',\n",
       " 'sum_electronegativity_inbetwen',\n",
       " 'sum_electronegativity_neghb',\n",
       " 'donor_groups_in_neighb',\n",
       " 'aceptor_groups_in_neighb',\n",
       " 'posIonizable_groups_in_neighb',\n",
       " 'aromatic_groups_in_neighb',\n",
       " 'hydrophobe_groups_in_neighb',\n",
       " 'lumpedHydrophobe_groups_in_neighb',\n",
       " 'negIonizable_groups_in_neighb',\n",
       " 'sigma_bonds',\n",
       " 'atom_1_type_smthmean',\n",
       " 'atom_1_hybridization_smthmean',\n",
       " 'pi_bonds_smthmean',\n",
       " 'graph_smile_smthmean',\n",
       " 'donor_groups_in_neighb_smthmean',\n",
       " 'aceptor_groups_in_neighb_smthmean',\n",
       " 'posIonizable_groups_in_neighb_smthmean',\n",
       " 'aromatic_groups_in_neighb_smthmean',\n",
       " 'hydrophobe_groups_in_neighb_smthmean',\n",
       " 'lumpedHydrophobe_groups_in_neighb_smthmean',\n",
       " 'negIonizable_groups_in_neighb_smthmean',\n",
       " 'sigma_bonds_smthmean']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = [\n",
    " 'type',\n",
    " 'atom_1_type',\n",
    " 'atom_1_hybridization',\n",
    " 'pi_bonds',\n",
    " 'graph_distance',\n",
    " 'graph_smile',\n",
    " 'angle',\n",
    " 'dihedral',\n",
    " 'sum_electronegativity_inbetwen',\n",
    " 'sum_electronegativity_neghb',\n",
    " 'donor_groups_in_neighb',\n",
    " 'aceptor_groups_in_neighb',\n",
    " 'posIonizable_groups_in_neighb',\n",
    " 'aromatic_groups_in_neighb',\n",
    " 'hydrophobe_groups_in_neighb',\n",
    " 'lumpedHydrophobe_groups_in_neighb',\n",
    " 'negIonizable_groups_in_neighb',\n",
    " 'sigma_bonds',\n",
    " 'atom_1_type_smthmean',\n",
    " 'atom_1_hybridization_smthmean',\n",
    " 'pi_bonds_smthmean',\n",
    " 'graph_smile_smthmean',\n",
    " 'donor_groups_in_neighb_smthmean',\n",
    " 'aceptor_groups_in_neighb_smthmean',\n",
    " 'posIonizable_groups_in_neighb_smthmean',\n",
    " 'aromatic_groups_in_neighb_smthmean',\n",
    " 'hydrophobe_groups_in_neighb_smthmean',\n",
    " 'lumpedHydrophobe_groups_in_neighb_smthmean',\n",
    " 'negIonizable_groups_in_neighb_smthmean',\n",
    " 'sigma_bonds_smthmean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505542"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the number of datapoints originally in the test set\n",
    "# which we need to predict. they all have scalar_coupling_constant as nan\n",
    "train['scalar_coupling_constant'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505542"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['scalar_coupling_constant'].iloc[-2505542:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[good_columns].iloc[:-2505542].copy()\n",
    "y_train = train['scalar_coupling_constant'].iloc[:-2505542]\n",
    "X_test = train[good_columns].iloc[-2505542:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(\"Fold %d started at %s\" %(fold_n + 1, time.ctime()))\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "                    \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(n_estimators=n_estimators, n_jobs=-1, **params)\n",
    "            model.fit(X_train, y_train, \n",
    "                      eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                      eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                      verbose=verbose, \n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, \n",
    "                              num_boost_round=20000, \n",
    "                              evals=watchlist, \n",
    "                              early_stopping_rounds=200, \n",
    "                              verbose_eval=verbose, \n",
    "                              params=params)\n",
    "            \n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), \n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            \n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), \n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(\"Fold {fold_n. {eval_metric}: {score:.4f}.}\".format(**locals()))\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=50000, \n",
    "                                      eval_metric=metrics_dict[eval_metric][\"catboost_metric_name\"],\n",
    "                                      loss_function=metrics_dict[eval_metric][\"catboost_metric_name\"])\n",
    "            model.set_params(**params)\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    result_dict['model'] = model\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "params = {'learning_rate': 0.03,\n",
    "          \"iterations\": 50000,\n",
    "          \"early_stopping_rounds\": 500,\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Tue Jul  9 12:03:36 2019\n",
      "Fold 2 started at Wed Jul 10 01:04:24 2019\n",
      "Fold 3 started at Wed Jul 10 10:34:56 2019\n",
      "Fold 4 started at Wed Jul 10 20:06:55 2019\n",
      "Fold 5 started at Thu Jul 11 05:39:19 2019\n",
      "CV mean score: 0.2230, std: 0.0023.\n"
     ]
    }
   ],
   "source": [
    "#This take a long while!!\n",
    "# Its a gradient boosting model using 5-folds validation strategy\n",
    "# I am uses 1500 estimators and eraly stopping after 200 rounds if no improvements \n",
    "result_dict_lgb = train_model_regression(X=X_train, X_test=X_test, y=y_train, \n",
    "                                         params=params, folds=folds, \n",
    "                                         model_type='lgb', eval_metric='group_mae', \n",
    "                                         plot_feature_importance=False, verbose=500, \n",
    "                                         early_stopping_rounds=200, n_estimators=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test[\"scalar_coupling_constant\"] = result_dict_lgb[\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['molecule_name','atom_index_0','atom_index_1','type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"submition_04.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
